import os
import numpy as np
import netCDF4 as nc
from netCDF4 import num2date
from sklearn.cluster import DBSCAN
from collections import Counter
import matplotlib
import streamlit as st
from datetime import datetime
import pandas as pd

# åŸºç¡€é…ç½®ï¼šé€‚é…Streamlitç¯å¢ƒ
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯ï¼Œé¿å…ç»˜å›¾å†²çª
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆè§£å†³Streamlitä¸­æ–‡ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100  # é€‚é…ç½‘é¡µæ˜¾ç¤ºåˆ†è¾¨ç‡


# --------------------------
# 1. Streamlité¡µé¢åˆå§‹åŒ–
# --------------------------
st.set_page_config(
    page_title="3D DBSCANæç«¯é™æ°´äº‹ä»¶è¯†åˆ«",
    page_icon="ğŸŒ§ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ä¾§è¾¹æ æ ‡é¢˜
st.sidebar.title("å‚æ•°é…ç½®")
st.sidebar.markdown("---")


# --------------------------
# 2. æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆä¿ç•™åŸé€»è¾‘ï¼Œé€‚é…Streamlitï¼‰
# --------------------------
def read_era5_data(file_obj):
    """è¯»å–ç”¨æˆ·ä¸Šä¼ çš„ERA5 netCDFæ–‡ä»¶ï¼ˆé€‚é…Streamlitæ–‡ä»¶å¯¹è±¡ï¼‰"""
    try:
        # ä»Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡è¯»å–ï¼Œæ— éœ€ä¿å­˜åˆ°æœ¬åœ°
        ds = nc.Dataset('temp.nc', mode='r', memory=file_obj.read())
        lon = ds.variables['longitude'][:]
        lat = ds.variables['latitude'][:]
        time = ds.variables['valid_time'][:]
        
        # è¯»å–é™æ°´å˜é‡ï¼ˆæ”¯æŒç”¨æˆ·é€‰æ‹©å˜é‡åï¼‰
        precip_var_name = st.session_state.get('precip_var_name', 'tp')
        if precip_var_name not in ds.variables:
            raise ValueError(f"æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å˜é‡ {precip_var_name}ï¼Œå¯ç”¨å˜é‡ï¼š{list(ds.variables.keys())}")
        
        tp = ds.variables[precip_var_name][:] * 1000  # è½¬æ¢ä¸ºmm/h
        time_units = ds.variables['valid_time'].units
        ds.close()
        return lon, lat, time, tp, time_units
    except Exception as e:
        st.error(f"æ•°æ®è¯»å–å¤±è´¥ï¼š{str(e)}")
        return None, None, None, None, None


def extract_extreme_precip(file_obj, config):
    """æå–æç«¯é™æ°´ç‚¹ï¼ˆé€‚é…StreamlitçŠ¶æ€ç®¡ç†ï¼‰"""
    with st.spinner("æ­£åœ¨è¯»å–å¹¶å¤„ç†ERA5æ•°æ®..."):
        lon, lat, time, tp, time_units = read_era5_data(file_obj)

        if lon is None:
            st.error("æ— æ³•è¯»å–æ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–å˜é‡å")
            return None

        # ç¨³å¥çš„æ—¶é—´è½¬æ¢ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
        try:
            time_datetime_py = num2date(time, units=time_units)
            time_datetime = np.array(time_datetime_py, dtype='datetime64[h]')
            st.success("æ—¶é—´è½¬æ¢æˆåŠŸï¼ˆä½¿ç”¨num2dateæ–¹æ³•ï¼‰")
        except Exception as e1:
            try:
                time_datetime = np.array(time, dtype='datetime64[h]')
                st.success("æ—¶é—´è½¬æ¢æˆåŠŸï¼ˆä½¿ç”¨ç›´æ¥è½¬æ¢æ–¹æ³•ï¼‰")
            except Exception as e2:
                try:
                    time_seconds = time.astype(float)
                    time_datetime = np.array('1970-01-01', dtype='datetime64[s]') + \
                                    np.array(time_seconds, dtype='timedelta64[s]')
                    time_datetime = time_datetime.astype('datetime64[h]')
                    st.success("æ—¶é—´è½¬æ¢æˆåŠŸï¼ˆä½¿ç”¨Unixæ—¶é—´æˆ³è§£æï¼‰")
                except Exception as e3:
                    st.error(
                        f"æ—¶é—´æ ¼å¼è½¬æ¢å¤±è´¥ï¼š\næ–¹æ³•1: {str(e1)}\næ–¹æ³•2: {str(e2)}\næ–¹æ³•3: {str(e3)}"
                    )
                    return None

        # è®¡ç®—ç›¸å¯¹äºèµ·å§‹æ—¶é—´çš„å¤©æ•°
        time_delta_hours = (time_datetime - time_datetime[0]).astype('timedelta64[h]').astype(int)
        time_days = time_delta_hours / 24.0

        # ç­›é€‰ç ”ç©¶åŒºåŸŸï¼ˆç”¨æˆ·å¯é…ç½®ï¼‰
        lon_mask = (lon >= config['domain'][0]) & (lon <= config['domain'][1])
        lat_mask = (lat >= config['domain'][2]) & (lat <= config['domain'][3])
        lon_indices = np.where(lon_mask)[0]
        lat_indices = np.where(lat_mask)[0]

        # æå–æç«¯é™æ°´ç‚¹
        st.info(f"æ ¹æ®é˜ˆå€¼ {config['unified_threshold']} mm/h ç­›é€‰æç«¯é™æ°´ç‚¹...")
        extreme_points = []

        if tp.ndim == 3:
            total_time_steps = tp.shape[0]
            progress_bar = st.progress(0, text="ç­›é€‰æç«¯ç‚¹ä¸­...")
            for t_idx in range(total_time_steps):
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.progress((t_idx + 1) / total_time_steps, 
                                     text=f"å¤„ç†æ—¶é—´æ­¥ {t_idx+1}/{total_time_steps}")
                
                time_slice = tp[t_idx, :, :]
                for lat_idx in lat_indices:
                    for lon_idx in lon_indices:
                        if time_slice[lat_idx, lon_idx] > config['unified_threshold']:
                            extreme_points.append([
                                lon[lon_idx],  # ç»åº¦
                                lat[lat_idx],  # çº¬åº¦
                                time_days[t_idx]
                            ])
            progress_bar.empty()

        if not extreme_points:
            st.warning(f"æœªæ£€æµ‹åˆ°æç«¯é™æ°´ç‚¹ï¼ˆå½“å‰é˜ˆå€¼: {config['unified_threshold']} mm/hï¼‰")
            return None
        st.success(f"å…±ç­›é€‰å‡º {len(extreme_points)} ä¸ªæç«¯é™æ°´ç‚¹")

        # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        st.session_state['data_dict'] = {
            'features': np.array(extreme_points, dtype=float),
            'raw_precip': tp,
            'lon': lon,
            'lat': lat,
            'time': time,
            'time_days': time_days,
            'base_datetime': time_datetime[0],
            'time_datetime': time_datetime
        }
        return st.session_state['data_dict']


# --------------------------
# 3. 3D DBSCANèšç±»ç±»ï¼ˆé€‚é…Streamlitå¯è§†åŒ–ï¼‰
# --------------------------
class ExtremeEventCluster:
    def __init__(self, config):
        self.config = config
        self.eps = config['dbscan_eps']
        self.min_pts = config['dbscan_min_pts']
        self.time_scale = config['time_scale']
        self.min_duration = config['min_duration']
        self.min_total_grid_count = config['min_total_grid_count']
        self.actual_time_eps = (self.eps / self.time_scale) * 24

        # ç»“æœä¿å­˜è·¯å¾„ï¼ˆStreamlitä¸­ä¼˜å…ˆå†…å­˜å±•ç¤ºï¼Œæ”¯æŒä¸‹è½½ï¼‰
        self.timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        self.result_key = f"results_{self.timestamp}"

    def cluster(self, data_dict):
        """æ‰§è¡Œèšç±»ï¼ˆæ ¸å¿ƒé€»è¾‘ä¸å˜ï¼Œæ·»åŠ StreamlitçŠ¶æ€æ›´æ–°ï¼‰"""
        with st.spinner("æ­£åœ¨æ‰§è¡Œ3D DBSCANèšç±»..."):
            features = data_dict['features'].copy()
            features[:, 2] = features[:, 2] * self.time_scale  # æ—¶é—´æ ‡å‡†åŒ–

            # æ‰§è¡ŒDBSCAN
            db = DBSCAN(eps=self.eps, min_samples=self.min_pts, metric='euclidean').fit(features)
            labels = db.labels_
            label_counts = Counter(labels)

            n_noise = label_counts.get(-1, 0)
            n_initial_events = len(label_counts) - 1 if -1 in label_counts else len(label_counts)
            
            # åœ¨é¡µé¢æ˜¾ç¤ºèšç±»åŸºç¡€ç»“æœ
            col1, col2, col3 = st.columns(3)
            col1.metric("åˆå§‹äº‹ä»¶æ•°", n_initial_events)
            col2.metric("å­¤ç«‹ç‚¹æ•°", n_noise)
            col3.metric("æ€»æç«¯ç‚¹æ•°", len(features))

            if n_initial_events == 0:
                st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆäº‹ä»¶ï¼Œè¯·è°ƒå°epsæˆ–min_ptså‚æ•°")
                return None

            # ç­›é€‰äº‹ä»¶ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
            st.subheader("äº‹ä»¶ç­›é€‰ï¼ˆåŸºäºæŒç»­æ—¶é—´å’Œå½±å“èŒƒå›´ï¼‰")
            qualified_events = {}
            event_id = 0
            filter_log = []

            for initial_id in range(n_initial_events):
                if initial_id not in label_counts:
                    continue

                event_mask = (labels == initial_id)
                event_features = data_dict['features'][event_mask]

                # æ£€æŸ¥æŒç»­æ—¶é—´
                time_vals = event_features[:, 2]
                duration_hours = (time_vals.max() - time_vals.min()) * 24 + 1
                # æ£€æŸ¥å½±å“ç½‘æ ¼æ•°
                grid_points = np.round(event_features[:, :2], 2)
                total_grid_count = len(np.unique(grid_points, axis=0))

                if duration_hours <= self.min_duration or total_grid_count < self.min_total_grid_count:
                    filter_log.append(
                        f"âŒ äº‹ä»¶{initial_id}ï¼šæŒç»­{duration_hours:.1f}h / ç½‘æ ¼{total_grid_count}ä¸ª â†’ ä¸æ»¡è¶³æ¡ä»¶"
                    )
                    continue

                qualified_events[event_id] = event_features
                filter_log.append(
                    f"âœ… äº‹ä»¶{event_id}ï¼šæŒç»­{duration_hours:.1f}h / ç½‘æ ¼{total_grid_count}ä¸ª â†’ ä¿ç•™"
                )
                event_id += 1

            # æ˜¾ç¤ºç­›é€‰æ—¥å¿—
            with st.expander("æŸ¥çœ‹ç­›é€‰è¯¦æƒ…", expanded=False):
                for log in filter_log:
                    st.write(log)

            n_qualified = len(qualified_events)
            st.success(f"äº‹ä»¶ç­›é€‰å®Œæˆï¼šå…±ä¿ç•™ {n_qualified} ä¸ªç¬¦åˆæ¡ä»¶çš„æç«¯é™æ°´äº‹ä»¶")

            if n_qualified == 0:
                st.warning("æœªä¿ç•™ä»»ä½•äº‹ä»¶ï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶ï¼ˆå¦‚é™ä½min_durationï¼‰")
                return None

            # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
            self.qualified_events = qualified_events
            st.session_state[self.result_key] = qualified_events
            st.session_state['cluster_obj'] = self  # ä¿å­˜èšç±»å¯¹è±¡ä¾›åç»­å¯è§†åŒ–
            return qualified_events

    def visualize_3d_spacetime(self, data_dict):
        """3Dæ—¶ç©ºå›¾ï¼ˆé€‚é…Streamlitæ˜¾ç¤ºï¼‰"""
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')

        # é¢œè‰²é…ç½®
        n_events = len(self.qualified_events)
        colors = plt.cm.tab20(np.linspace(0, 1, min(n_events, 20)))
        if n_events > 20:
            colors = np.tile(colors, (n_events // 20 + 1, 1))[:n_events]

        # ç»˜åˆ¶æ¯ä¸ªäº‹ä»¶
        for event_idx, (event_id, event_features) in enumerate(self.qualified_events.items()):
            ax.scatter(
                event_features[:, 1],  # Xè½´ï¼šçº¬åº¦
                event_features[:, 0],  # Yè½´ï¼šç»åº¦
                event_features[:, 2],  # Zè½´ï¼šæ—¶é—´
                color=colors[event_idx],
                label=f'äº‹ä»¶ {event_id}',
                alpha=0.7,
                s=30
            )

        # åæ ‡è½´è®¾ç½®
        ax.set_xlim([self.config['domain'][3], self.config['domain'][2]])  # çº¬åº¦ï¼šå³åˆ°å·¦é€’å¢
        ax.set_xlabel('çº¬åº¦ (Â°N)', fontsize=11)
        ax.set_ylim([self.config['domain'][0], self.config['domain'][1]])  # ç»åº¦ï¼šå·¦åˆ°å³é€’å¢
        ax.set_ylabel('ç»åº¦ (Â°E)', fontsize=11)

        # æ—¶é—´è½´
        z_min, z_max = data_dict['time_days'].min(), data_dict['time_days'].max()
        z_ticks = np.arange(np.floor(z_min), np.ceil(z_max) + 1, 1)
        z_tick_labels = [f'{int(day)}æ—¥' for day in z_ticks]
        ax.set_zlim([z_min, z_max])
        ax.set_zticks(z_ticks)
        ax.set_zticklabels(z_tick_labels)
        ax.set_zlabel('æ—¶é—´', fontsize=11)

        # æ ‡é¢˜å’Œå›¾ä¾‹
        plt.title(
            f'æç«¯é™æ°´äº‹ä»¶3Dæ—¶ç©ºåˆ†å¸ƒï¼ˆå…±{n_events}ä¸ªäº‹ä»¶ï¼‰',
            fontsize=14, pad=20
        )
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)

        # åœ¨Streamlitä¸­æ˜¾ç¤ºå›¾ç‰‡
        st.pyplot(fig)
        plt.close()

        # æä¾›ä¸‹è½½åŠŸèƒ½
        self.save_fig_to_download(fig, "3d_spacetime_distribution.png")

    def visualize_2d(self, data_dict, plot_type):
        """é€šç”¨2Då¯è§†åŒ–ï¼ˆç»åº¦-æ—¶é—´/çº¬åº¦-æ—¶é—´/ç©ºé—´æŠ•å½±ï¼‰"""
        fig = plt.figure(figsize=(12, 6))
        ax = fig.add_subplot(111)

        # é¢œè‰²é…ç½®
        n_events = len(self.qualified_events)
        colors = plt.cm.tab20(np.linspace(0, 1, min(n_events, 20)))
        if n_events > 20:
            colors = np.tile(colors, (n_events // 20 + 1, 1))[:n_events]

        # ç»˜åˆ¶æ¯ä¸ªäº‹ä»¶
        for event_idx, (event_id, event_features) in enumerate(self.qualified_events.items()):
            if plot_type == 'lon_time':
                x, y = event_features[:, 0], event_features[:, 2]
                x_label, y_label = 'ç»åº¦ (Â°E)', 'æ—¶é—´'
                title = f'æç«¯é™æ°´äº‹ä»¶ç»åº¦-æ—¶é—´åˆ†å¸ƒï¼ˆå…±{n_events}ä¸ªäº‹ä»¶ï¼‰'
                x_lim = [self.config['domain'][0], self.config['domain'][1]]
            elif plot_type == 'lat_time':
                x, y = event_features[:, 1], event_features[:, 2]
                x_label, y_label = 'çº¬åº¦ (Â°N)', 'æ—¶é—´'
                title = f'æç«¯é™æ°´äº‹ä»¶çº¬åº¦-æ—¶é—´åˆ†å¸ƒï¼ˆå…±{n_events}ä¸ªäº‹ä»¶ï¼‰'
                x_lim = [self.config['domain'][2], self.config['domain'][3]]
            elif plot_type == 'spatial':
                x, y = event_features[:, 0], event_features[:, 1]
                x_label, y_label = 'ç»åº¦ (Â°E)', 'çº¬åº¦ (Â°N)'
                title = f'æç«¯é™æ°´äº‹ä»¶ç©ºé—´æŠ•å½±ï¼ˆå…±{n_events}ä¸ªäº‹ä»¶ï¼‰'
                x_lim = [self.config['domain'][0], self.config['domain'][1]]
                y_lim = [self.config['domain'][2], self.config['domain'][3]]
            else:
                return

            ax.scatter(x, y, color=colors[event_idx], label=f'äº‹ä»¶ {event_id}', alpha=0.7, s=30)

        # åæ ‡è½´é…ç½®
        ax.set_xlim(x_lim)
        ax.set_xlabel(x_label, fontsize=11)
        ax.set_ylabel(y_label, fontsize=11)
        if plot_type == 'spatial':
            ax.set_ylim(y_lim)
        else:
            # æ—¶é—´è½´é…ç½®
            z_min, z_max = data_dict['time_days'].min(), data_dict['time_days'].max()
            z_ticks = np.arange(np.floor(z_min), np.ceil(z_max) + 1, 1)
            z_tick_labels = [f'{int(day)}æ—¥' for day in z_ticks]
            ax.set_ylim([z_min, z_max])
            ax.set_yticks(z_ticks)
            ax.set_yticklabels(z_tick_labels)

        # ç½‘æ ¼å’Œå›¾ä¾‹
        ax.grid(True, linestyle='--', alpha=0.7, color='gray')
        plt.title(title, fontsize=14, pad=20)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
        plt.tight_layout()

        # åœ¨Streamlitä¸­æ˜¾ç¤º
        st.pyplot(fig)
        plt.close()

        # æä¾›ä¸‹è½½
        filename = f"{plot_type}_distribution.png"
        self.save_fig_to_download(fig, filename)

    def save_fig_to_download(self, fig, filename):
        """å°†å›¾ç‰‡ä¿å­˜åˆ°å†…å­˜ï¼Œæä¾›Streamlitä¸‹è½½æŒ‰é’®"""
        import io
        buf = io.BytesIO()
        fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        
        # æ·»åŠ ä¸‹è½½æŒ‰é’®
        st.download_button(
            label=f"ä¸‹è½½ {filename}",
            data=buf,
            file_name=filename,
            mime="image/png",
            key=f"download_{filename}_{self.timestamp}"
        )

    def export_events_to_csv(self):
        """å¯¼å‡ºäº‹ä»¶è¯¦æƒ…ä¸ºCSVï¼ˆæ”¯æŒæ‰¹é‡ä¸‹è½½ï¼‰"""
        all_events_df = pd.DataFrame()
        for event_id, event_features in self.qualified_events.items():
            # æ„å»ºäº‹ä»¶DataFrame
            df = pd.DataFrame({
                'longitude': event_features[:, 0].round(2),
                'latitude': event_features[:, 1].round(2),
                'time_days': event_features[:, 2].round(2),
                'event_id': event_id
            })
            # è½¬æ¢æ—¶é—´ä¸ºå¯è¯»æ ¼å¼
            base_dt = self.config['base_datetime']
            df['datetime'] = [base_dt + np.timedelta64(int(day*24), 'h') for day in df['time_days']]
            all_events_df = pd.concat([all_events_df, df], ignore_index=True)

        # ä¿å­˜åˆ°å†…å­˜
        import io
        buf = io.StringIO()
        all_events_df.to_csv(buf, index=False, encoding='utf-8-sig')
        buf.seek(0)

        # ä¸‹è½½æŒ‰é’®
        st.download_button(
            label=f"ä¸‹è½½æ‰€æœ‰äº‹ä»¶è¯¦æƒ…ï¼ˆCSVï¼‰",
            data=buf,
            file_name=f"extreme_rain_events_{self.timestamp}.csv",
            mime="text/csv",
            key=f"download_csv_{self.timestamp}"
        )
        return all_events_df


# --------------------------
# 4. Streamlitäº¤äº’å¼ç•Œé¢ï¼ˆæ ¸å¿ƒæµç¨‹ï¼‰
# --------------------------
def main():
    # ä¸»æ ‡é¢˜
    st.title("ğŸŒ§ï¸ 3D DBSCANæç«¯é™æ°´äº‹ä»¶è¯†åˆ«ç³»ç»Ÿ")
    st.markdown("åŸºäºERA5å°æ—¶é™æ°´æ•°æ®ï¼Œé€šè¿‡3D DBSCANç®—æ³•è¯†åˆ«æ—¶ç©ºè¿ç»­çš„æç«¯é™æ°´äº‹ä»¶")
    st.markdown("---")

    # æ­¥éª¤1ï¼šä¸Šä¼ æ•°æ®
    st.subheader("æ­¥éª¤1ï¼šä¸Šä¼ ERA5 NetCDFæ•°æ®")
    uploaded_file = st.file_uploader("é€‰æ‹©NCæ–‡ä»¶ï¼ˆæ”¯æŒERA5é™æ°´æ•°æ®ï¼‰", type=["nc", "netcdf"])
    
    # æ­¥éª¤2ï¼šé…ç½®å‚æ•°ï¼ˆä¾§è¾¹æ ï¼‰
    st.sidebar.subheader("æ•°æ®å‚æ•°")
    # é™æ°´å˜é‡åï¼ˆé»˜è®¤tpï¼Œç”¨æˆ·å¯ä¿®æ”¹ï¼‰
    precip_var_name = st.sidebar.text_input("é™æ°´å˜é‡å", value="tp", key="precip_var_name")
    # ç ”ç©¶åŒºåŸŸï¼ˆé»˜è®¤ä¸­å›½åŒºåŸŸï¼‰
    st.sidebar.subheader("ç ”ç©¶åŒºåŸŸ")
    domain = {
        'min_lon': st.sidebar.number_input("æœ€å°ç»åº¦", value=73.0, step=0.5),
        'max_lon': st.sidebar.number_input("æœ€å¤§ç»åº¦", value=135.0, step=0.5),
        'min_lat': st.sidebar.number_input("æœ€å°çº¬åº¦", value=3.0, step=0.5),
        'max_lat': st.sidebar.number_input("æœ€å¤§çº¬åº¦", value=54.0, step=0.5)
    }

    # DBSCANå‚æ•°
    st.sidebar.subheader("DBSCANèšç±»å‚æ•°")
    dbscan_params = {
        'unified_threshold': st.sidebar.slider("æç«¯é™æ°´é˜ˆå€¼ï¼ˆmm/hï¼‰", min_value=1.0, max_value=50.0, value=20.0, step=0.5),
        'dbscan_eps': st.sidebar.slider("EPSï¼ˆç©ºé—´é‚»åŸŸï¼Œåº¦ï¼‰", min_value=1.0, max_value=10.0, value=5.0, step=0.1),
        'dbscan_min_pts': st.sidebar.slider("MIN_PTSï¼ˆæ ¸å¿ƒç‚¹æœ€å°ç‚¹æ•°ï¼‰", min_value=3, max_value=30, value=10, step=1),
        'time_scale': st.sidebar.slider("æ—¶é—´ç¼©æ”¾ç³»æ•°", min_value=0.1, max_value=5.0, value=1.0, step=0.1)
    }

    # äº‹ä»¶ç­›é€‰å‚æ•°
    st.sidebar.subheader("äº‹ä»¶ç­›é€‰æ¡ä»¶")
    filter_params = {
        'min_duration': st.sidebar.number_input("æœ€å°æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰", min_value=6, max_value=100, value=48, step=6),
        'min_total_grid_count': st.sidebar.number_input("æœ€å°å½±å“ç½‘æ ¼æ•°", min_value=10, max_value=200, value=50, step=5)
    }

    # åˆå¹¶æ‰€æœ‰é…ç½®
    config = {**domain, **dbscan_params, **filter_params}

    # æ­¥éª¤3ï¼šå¤„ç†æ•°æ®ï¼ˆæç«¯ç‚¹æå–ï¼‰
    if uploaded_file is not None:
        st.subheader("æ­¥éª¤2ï¼šæå–æç«¯é™æ°´ç‚¹")
        if st.button("å¼€å§‹æå–æç«¯ç‚¹", key="extract_btn"):
            # é‡ç½®ä¹‹å‰çš„ç»“æœ
            st.session_state.pop('data_dict', None)
            st.session_state.pop('cluster_obj', None)
            
            # æ‰§è¡Œæç«¯ç‚¹æå–
            data_dict = extract_extreme_precip(uploaded_file, config)

    # æ­¥éª¤4ï¼šèšç±»åˆ†æï¼ˆæ•°æ®æå–å®Œæˆåæ˜¾ç¤ºï¼‰
    if 'data_dict' in st.session_state:
        st.markdown("---")
        st.subheader("æ­¥éª¤3ï¼š3D DBSCANèšç±»åˆ†æ")
        data_dict = st.session_state['data_dict']
        
        if st.button("å¼€å§‹èšç±»", key="cluster_btn"):
            # åˆ›å»ºèšç±»å¯¹è±¡å¹¶æ‰§è¡Œ
            cluster_obj = ExtremeEventCluster(config)
            qualified_events = cluster_obj.cluster(data_dict)
            
            # èšç±»å®Œæˆåæ˜¾ç¤ºå¯è§†åŒ–
            if qualified_events is not None:
                st.markdown("---")
                st.subheader("æ­¥éª¤4ï¼šç»“æœå¯è§†åŒ–")
                
                # 3Dæ—¶ç©ºå›¾
                st.subheader("3Dæ—¶ç©ºåˆ†å¸ƒ")
                cluster_obj.visualize_3d_spacetime(data_dict)
                
                # 2Då›¾è¡¨ï¼ˆåˆ†æ æ˜¾ç¤ºï¼‰
                st.subheader("2Dåˆ†å¸ƒå›¾è¡¨")
                tab1, tab2, tab3 = st.tabs(["ç»åº¦-æ—¶é—´", "çº¬åº¦-æ—¶é—´", "ç©ºé—´æŠ•å½±"])
                with tab1:
                    cluster_obj.visualize_2d(data_dict, 'lon_time')
                with tab2:
                    cluster_obj.visualize_2d(data_dict, 'lat_time')
                with tab3:
                    cluster_obj.visualize_2d(data_dict, 'spatial')
                
                # å¯¼å‡ºç»“æœ
                st.markdown("---")
                st.subheader("æ­¥éª¤5ï¼šå¯¼å‡ºç»“æœ")
                cluster_obj.export_events_to_csv()


if __name__ == "__main__":
    main()
